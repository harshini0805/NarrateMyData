{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a03fbb3-6e6f-418c-94bf-87dd1a169050",
   "metadata": {},
   "source": [
    "### Libraries needed for data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2aebe999-314a-4a91-b649-c7c90eef16b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9512eebe-b7d8-49bf-b492-30db09d294f2",
   "metadata": {},
   "source": [
    "### Function to read the csv uploaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "453042c4-f90f-439b-98ba-6dc01adbb27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    import pandas as pd\n",
    "    try:\n",
    "        dataset = input(\"Enter the name of the dataset: \")\n",
    "        df = pd.read_csv(dataset)\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a305a591-f73b-4659-827a-20f4e36dc814",
   "metadata": {},
   "source": [
    "### Function to check the sanity of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "919b94f4-872c-4488-81e6-acd4d292cd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check(df):\n",
    "    if df is None:\n",
    "        print(\"Data doesn't exist.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        print(\"Shape of the dataset:\", df.shape)\n",
    "        print(\"Dataset Info:\")\n",
    "        df.info()\n",
    "        print(\"Missing values percentage per column:\")\n",
    "        print((df.isnull().sum() / df.shape[0] * 100).round(2))\n",
    "\n",
    "        print(\"Duplicate rows count:\", df.duplicated().sum())\n",
    "    except Exception as e:\n",
    "        print(f\"Error during sanity check: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eb9d28-147e-4f39-b461-cbe660ce4176",
   "metadata": {},
   "source": [
    "### Checking for catergorical distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5038097-9f00-41da-afa9-8e5973a248cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_categorical_distributions(df):\n",
    "    obj_cols = df.select_dtypes(include=\"object\").columns\n",
    "    if len(obj_cols) == 0:\n",
    "        print(\"No categorical (object) columns found.\")\n",
    "        return\n",
    "\n",
    "    for col in obj_cols:\n",
    "        unique_vals = df[col].nunique()\n",
    "        print(f\"\\n'{col}' ‚Äî {unique_vals} unique value(s)\")\n",
    "\n",
    "        if unique_vals <= 10:\n",
    "            print(df[col].value_counts(dropna=False))\n",
    "        else:\n",
    "            print(\"Too many unique values to display.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a037fc88-3b67-429d-870f-9ef157c42bf2",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8be8ae47-5332-449c-a936-be517b6c36cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eda(df):\n",
    "    if df is None: \n",
    "        print(\"Data doesn't exist.\")\n",
    "        return \n",
    "    try: \n",
    "        print(\"Description of the dataframe: \")\n",
    "        print(df.describe())\n",
    "        print(\"Decription of the categorical data present: \")\n",
    "        print(df.describe(include = \"object\"))\n",
    "    except Exception as e:\n",
    "        print(f\"Exception {e} occured.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ab22475-1180-438b-9df9-2dc6a105a29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def choose_imputer(df):\n",
    "    num_cols = df.select_dtypes(include=\"number\").columns\n",
    "    cat_cols = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns\n",
    "\n",
    "    if len(cat_cols) > len(num_cols):\n",
    "        return SimpleImputer(strategy=\"most_frequent\")\n",
    "    else:\n",
    "        return SimpleImputer(strategy=\"mean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8c7c99-bb35-4860-a5cd-06ac45483c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = choose_imputer(df)\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b580bcd9-31f1-4bd3-be35-50ff652212e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outlier_columns(df):\n",
    "    outlier_cols = []\n",
    "\n",
    "    for col in df.select_dtypes(include=\"number\").columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "\n",
    "        # If more than, say, 1% of values are outliers, consider treating\n",
    "        if len(outliers) / df.shape[0] > 0.01:\n",
    "            outlier_cols.append(col)\n",
    "\n",
    "    return outlier_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1e7e2c-0295-4917-994d-2e84ba4babba",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_outliers = detect_outlier_columns(df)\n",
    "\n",
    "if columns_with_outliers:\n",
    "    print(f\"‚ö†Ô∏è Consider treating outliers in: {columns_with_outliers}\")\n",
    "else:\n",
    "    print(\"‚úÖ No significant outliers found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ad97f9-a150-4b8c-bf70-0b111605c4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "df_scaled = scaler.fit_transform(df[numerical_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18b7d3d-1d15-4730-9450-fc346d989bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler\n",
    "\n",
    "def choose_scaler(df):\n",
    "    # Detect outliers using IQR rule\n",
    "    def has_outliers(series):\n",
    "        Q1 = series.quantile(0.25)\n",
    "        Q3 = series.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "        return ((series < lower) | (series > upper)).any()\n",
    "    \n",
    "    numerical_cols = df.select_dtypes(include=\"number\").columns\n",
    "    \n",
    "    # If many columns have outliers, use RobustScaler\n",
    "    outlier_cols = [col for col in numerical_cols if has_outliers(df[col])]\n",
    "    if len(outlier_cols) > len(numerical_cols) / 2:\n",
    "        return RobustScaler()\n",
    "    \n",
    "    # If data is sparse or has negatives but no centering desired (example condition)\n",
    "    # (You can customize this condition for your use case)\n",
    "    if (df[numerical_cols] < 0).any().any():\n",
    "        return MaxAbsScaler()\n",
    "    \n",
    "    # Else, if values are mostly positive and no big outliers, MinMaxScaler or StandardScaler\n",
    "    # You can decide between these based on skewness or domain knowledge\n",
    "    skewed_cols = [col for col in numerical_cols if abs(df[col].skew()) > 1]\n",
    "    if len(skewed_cols) > len(numerical_cols) / 2:\n",
    "        return MinMaxScaler()\n",
    "    \n",
    "    return StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a35266-1ccb-4626-96a1-7e17bd4bbd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder, MultiLabelBinarizer\n",
    "\n",
    "def auto_encode(df, target_cols=None, multilabel_cols=None, ordinal_mappings=None, max_onehot=15):\n",
    "    \"\"\"\n",
    "    Auto encodes columns in df based on their type and provided info.\n",
    "    \n",
    "    Params:\n",
    "    - df: input DataFrame\n",
    "    - target_cols: list of columns to label encode (usually target variables)\n",
    "    - multilabel_cols: list of columns containing multi-label data (iterables per cell)\n",
    "    - ordinal_mappings: dict {col_name: ordered list of categories}\n",
    "    - max_onehot: max unique categories to use OneHot encoding\n",
    "    \n",
    "    Returns:\n",
    "    - df_encoded: transformed DataFrame\n",
    "    - encoders: dict {col_name: fitted encoder object}\n",
    "    \"\"\"\n",
    "    target_cols = target_cols or []\n",
    "    multilabel_cols = multilabel_cols or []\n",
    "    ordinal_mappings = ordinal_mappings or {}\n",
    "    \n",
    "    df_encoded = df.copy()\n",
    "    encoders = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col in target_cols:\n",
    "            le = LabelEncoder()\n",
    "            df_encoded[col] = le.fit_transform(df[col])\n",
    "            encoders[col] = le\n",
    "            continue\n",
    "        \n",
    "        if col in multilabel_cols:\n",
    "            mlb = MultiLabelBinarizer()\n",
    "            encoded = mlb.fit_transform(df[col])\n",
    "            mlb_df = pd.DataFrame(encoded, columns=[f\"{col}_{cls}\" for cls in mlb.classes_], index=df.index)\n",
    "            df_encoded = pd.concat([df_encoded.drop(columns=[col]), mlb_df], axis=1)\n",
    "            encoders[col] = mlb\n",
    "            continue\n",
    "        \n",
    "        dtype = df[col].dtype\n",
    "        n_unique = df[col].nunique()\n",
    "        \n",
    "        if col in ordinal_mappings:\n",
    "            categories = [ordinal_mappings[col]]\n",
    "            oe = OrdinalEncoder(categories=categories)\n",
    "            df_encoded[col] = oe.fit_transform(df[[col]])\n",
    "            encoders[col] = oe\n",
    "            continue\n",
    "        \n",
    "        if dtype.name in ['object', 'category', 'bool']:\n",
    "            if n_unique <= max_onehot:\n",
    "                ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "                encoded = ohe.fit_transform(df[[col]])\n",
    "                ohe_df = pd.DataFrame(encoded, columns=[f\"{col}_{cat}\" for cat in ohe.categories_[0]], index=df.index)\n",
    "                df_encoded = pd.concat([df_encoded.drop(columns=[col]), ohe_df], axis=1)\n",
    "                encoders[col] = ohe\n",
    "            else:\n",
    "                # fallback: ordinal encoding for high-cardinality nominal\n",
    "                oe = OrdinalEncoder()\n",
    "                df_encoded[col] = oe.fit_transform(df[[col]])\n",
    "                encoders[col] = oe\n",
    "        \n",
    "        else:\n",
    "            # leave numeric columns as-is or add numeric encoding later\n",
    "            pass\n",
    "    \n",
    "    return df_encoded, encoders\n",
    "\n",
    "\n",
    "def transform_new(df_new, encoders):\n",
    "    \"\"\"\n",
    "    Use fitted encoders to transform new dataframe\n",
    "    \n",
    "    Params:\n",
    "    - df_new: new DataFrame\n",
    "    - encoders: dict of fitted encoders from auto_encode()\n",
    "    \n",
    "    Returns:\n",
    "    - transformed DataFrame\n",
    "    \"\"\"\n",
    "    df_transformed = df_new.copy()\n",
    "    \n",
    "    for col, encoder in encoders.items():\n",
    "        if isinstance(encoder, LabelEncoder):\n",
    "            df_transformed[col] = encoder.transform(df_new[col])\n",
    "        elif isinstance(encoder, MultiLabelBinarizer):\n",
    "            encoded = encoder.transform(df_new[col])\n",
    "            mlb_df = pd.DataFrame(encoded, columns=[f\"{col}_{cls}\" for cls in encoder.classes_], index=df_new.index)\n",
    "            df_transformed = pd.concat([df_transformed.drop(columns=[col]), mlb_df], axis=1)\n",
    "        elif isinstance(encoder, OneHotEncoder):\n",
    "            encoded = encoder.transform(df_new[[col]])\n",
    "            ohe_df = pd.DataFrame(encoded, columns=[f\"{col}_{cat}\" for cat in encoder.categories_[0]], index=df_new.index)\n",
    "            df_transformed = pd.concat([df_transformed.drop(columns=[col]), ohe_df], axis=1)\n",
    "        elif isinstance(encoder, OrdinalEncoder):\n",
    "            df_transformed[col] = encoder.transform(df_new[[col]])\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    return df_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31a2aafb-3f51-49f4-bc0c-56d21bb8aa0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Data Preprocessing Pipeline...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the name of the dataset:  iris.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully loaded dataset: iris.csv\n",
      "\n",
      "============================================================\n",
      "üìä DATA SANITY CHECK REPORT\n",
      "============================================================\n",
      "Shape of the dataset: (150, 5)\n",
      "Memory usage: 0.01 MB\n",
      "\n",
      "üìã Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sepal_length  150 non-null    float64\n",
      " 1   sepal_width   150 non-null    float64\n",
      " 2   petal_length  150 non-null    float64\n",
      " 3   petal_width   150 non-null    float64\n",
      " 4   species       150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n",
      "\n",
      "üîç Missing Values Analysis:\n",
      "‚úÖ No missing values found!\n",
      "\n",
      "üîÑ Duplicate rows: 3\n",
      "   (2.00% of total data)\n",
      "\n",
      "============================================================\n",
      "üìà CATEGORICAL DISTRIBUTIONS\n",
      "============================================================\n",
      "\n",
      "'species' ‚Äî 3 unique value(s)\n",
      "species\n",
      "setosa        50\n",
      "versicolor    50\n",
      "virginica     50\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "üìä EXPLORATORY DATA ANALYSIS\n",
      "============================================================\n",
      "\n",
      "üî¢ Numerical Columns Summary:\n",
      "       sepal_length  sepal_width  petal_length  petal_width\n",
      "count        150.00       150.00        150.00       150.00\n",
      "mean           5.84         3.05          3.76         1.20\n",
      "std            0.83         0.43          1.76         0.76\n",
      "min            4.30         2.00          1.00         0.10\n",
      "25%            5.10         2.80          1.60         0.30\n",
      "50%            5.80         3.00          4.35         1.30\n",
      "75%            6.40         3.30          5.10         1.80\n",
      "max            7.90         4.40          6.90         2.50\n",
      "\n",
      "üìù Categorical Columns Summary:\n",
      "       species\n",
      "count      150\n",
      "unique       3\n",
      "top     setosa\n",
      "freq        50\n",
      "üóëÔ∏è Removed 3 duplicate rows\n",
      "‚úÖ No missing values to handle!\n",
      "\n",
      "‚ö†Ô∏è Outlier Detection Results:\n",
      "     Column  Outlier Count  Outlier %\n",
      "sepal_width              4       2.72\n",
      "\n",
      "üéØ Chosen scaler: StandardScaler (normal distribution assumed)\n",
      "   ‚úÖ Scaled 4 numerical columns\n",
      "\n",
      "üî§ Encoding 1 categorical columns...\n",
      "   ‚úÖ One-hot encoded 'species' (3 categories)\n",
      "\n",
      "============================================================\n",
      "‚úÖ PREPROCESSING COMPLETED!\n",
      "============================================================\n",
      "Final dataset shape: (147, 6)\n",
      "Final columns: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species_versicolor', 'species_virginica']\n",
      "\n",
      "üéâ Data preprocessing successful!\n",
      "Your data is now ready for machine learning!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.imputer = None\n",
    "        self.scaler = None\n",
    "        self.encoders = {}\n",
    "        self.numerical_cols = []\n",
    "        self.categorical_cols = []\n",
    "        \n",
    "    def get_dataset(self):\n",
    "        \"\"\"Load dataset from CSV file.\"\"\"\n",
    "        try:\n",
    "            dataset = input(\"Enter the name of the dataset: \")\n",
    "            df = pd.read_csv(dataset)\n",
    "            print(f\"‚úÖ Successfully loaded dataset: {dataset}\")\n",
    "            return df\n",
    "        except FileNotFoundError:\n",
    "            print(\"‚ùå File not found. Please check the filename.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "    def sanity_check(self, df):\n",
    "        \"\"\"Perform comprehensive data quality checks.\"\"\"\n",
    "        if df is None:\n",
    "            print(\"‚ùå Data doesn't exist.\")\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"üìä DATA SANITY CHECK REPORT\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            # Basic info\n",
    "            print(f\"Shape of the dataset: {df.shape}\")\n",
    "            print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "            \n",
    "            print(\"\\nüìã Dataset Info:\")\n",
    "            df.info()\n",
    "            \n",
    "            # Missing values\n",
    "            print(\"\\nüîç Missing Values Analysis:\")\n",
    "            missing_percent = (df.isnull().sum() / df.shape[0] * 100).round(2)\n",
    "            missing_data = pd.DataFrame({\n",
    "                'Column': missing_percent.index,\n",
    "                'Missing Count': df.isnull().sum(),\n",
    "                'Missing %': missing_percent\n",
    "            })\n",
    "            missing_data = missing_data[missing_data['Missing Count'] > 0]\n",
    "            \n",
    "            if len(missing_data) > 0:\n",
    "                print(missing_data.to_string(index=False))\n",
    "            else:\n",
    "                print(\"‚úÖ No missing values found!\")\n",
    "            \n",
    "            # Duplicates\n",
    "            duplicate_count = df.duplicated().sum()\n",
    "            print(f\"\\nüîÑ Duplicate rows: {duplicate_count}\")\n",
    "            if duplicate_count > 0:\n",
    "                print(f\"   ({duplicate_count/len(df)*100:.2f}% of total data)\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during sanity check: {e}\")\n",
    "            return False\n",
    "\n",
    "    def check_categorical_distributions(self, df):\n",
    "        \"\"\"Analyze categorical column distributions.\"\"\"\n",
    "        if df is None:\n",
    "            return\n",
    "            \n",
    "        obj_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "        if len(obj_cols) == 0:\n",
    "            print(\"‚ÑπÔ∏è No categorical columns found.\")\n",
    "            return\n",
    "            \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìà CATEGORICAL DISTRIBUTIONS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for col in obj_cols:\n",
    "            unique_vals = df[col].nunique()\n",
    "            print(f\"\\n'{col}' ‚Äî {unique_vals} unique value(s)\")\n",
    "            \n",
    "            if unique_vals <= 15:  # Show distributions for low cardinality\n",
    "                print(df[col].value_counts(dropna=False).head(10))\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è High cardinality - showing top 5 values:\")\n",
    "                print(df[col].value_counts().head(5))\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "    def eda_summary(self, df):\n",
    "        \"\"\"Generate exploratory data analysis summary.\"\"\"\n",
    "        if df is None: \n",
    "            print(\"‚ùå Data doesn't exist.\")\n",
    "            return \n",
    "            \n",
    "        try:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"üìä EXPLORATORY DATA ANALYSIS\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            # Numerical summary\n",
    "            numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "            if len(numerical_cols) > 0:\n",
    "                print(\"\\nüî¢ Numerical Columns Summary:\")\n",
    "                print(df[numerical_cols].describe().round(2))\n",
    "            \n",
    "            # Categorical summary\n",
    "            categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "            if len(categorical_cols) > 0:\n",
    "                print(\"\\nüìù Categorical Columns Summary:\")\n",
    "                print(df[categorical_cols].describe())\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Exception occurred: {e}\")\n",
    "\n",
    "    def handle_missing_values(self, df):\n",
    "        \"\"\"Handle missing values using appropriate strategies.\"\"\"\n",
    "        if df.isnull().sum().sum() == 0:\n",
    "            print(\"‚úÖ No missing values to handle!\")\n",
    "            return df\n",
    "            \n",
    "        print(\"\\nüîß Handling Missing Values...\")\n",
    "        \n",
    "        # Separate numerical and categorical columns\n",
    "        self.numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        self.categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "        \n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # Handle numerical columns\n",
    "        if self.numerical_cols:\n",
    "            num_imputer = SimpleImputer(strategy='median')  # More robust than mean\n",
    "            df_processed[self.numerical_cols] = num_imputer.fit_transform(df[self.numerical_cols])\n",
    "            print(f\"   ‚úÖ Imputed {len(self.numerical_cols)} numerical columns with median\")\n",
    "        \n",
    "        # Handle categorical columns\n",
    "        if self.categorical_cols:\n",
    "            cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "            df_processed[self.categorical_cols] = cat_imputer.fit_transform(df[self.categorical_cols])\n",
    "            print(f\"   ‚úÖ Imputed {len(self.categorical_cols)} categorical columns with mode\")\n",
    "        \n",
    "        return df_processed\n",
    "\n",
    "    def detect_and_report_outliers(self, df):\n",
    "        \"\"\"Detect outliers using IQR method.\"\"\"\n",
    "        outlier_summary = []\n",
    "        \n",
    "        for col in df.select_dtypes(include=[np.number]).columns:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "            outlier_percentage = len(outliers) / len(df) * 100\n",
    "            \n",
    "            if outlier_percentage > 1:  # More than 1% outliers\n",
    "                outlier_summary.append({\n",
    "                    'Column': col,\n",
    "                    'Outlier Count': len(outliers),\n",
    "                    'Outlier %': round(outlier_percentage, 2)\n",
    "                })\n",
    "        \n",
    "        if outlier_summary:\n",
    "            print(\"\\n‚ö†Ô∏è Outlier Detection Results:\")\n",
    "            outlier_df = pd.DataFrame(outlier_summary)\n",
    "            print(outlier_df.to_string(index=False))\n",
    "            return [item['Column'] for item in outlier_summary]\n",
    "        else:\n",
    "            print(\"‚úÖ No significant outliers found.\")\n",
    "            return []\n",
    "\n",
    "    def choose_and_apply_scaler(self, df):\n",
    "        \"\"\"Choose appropriate scaler based on data characteristics.\"\"\"\n",
    "        numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        if len(numerical_cols) == 0:\n",
    "            print(\"‚ÑπÔ∏è No numerical columns to scale.\")\n",
    "            return df\n",
    "        \n",
    "        # Analyze data characteristics\n",
    "        outlier_cols = []\n",
    "        sparse_cols = []\n",
    "        skewed_cols = []\n",
    "        \n",
    "        for col in numerical_cols:\n",
    "            # Check for outliers\n",
    "            Q1, Q3 = df[col].quantile([0.25, 0.75])\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()\n",
    "            if outliers / len(df) > 0.1:  # >10% outliers\n",
    "                outlier_cols.append(col)\n",
    "            \n",
    "            # Check for sparsity (zeros)\n",
    "            if (df[col] == 0).sum() / len(df) > 0.5:  # >50% zeros\n",
    "                sparse_cols.append(col)\n",
    "            \n",
    "            # Check for skewness\n",
    "            if abs(df[col].skew()) > 2:\n",
    "                skewed_cols.append(col)\n",
    "        \n",
    "        # Choose scaler based on analysis\n",
    "        if len(sparse_cols) > len(numerical_cols) / 2:\n",
    "            self.scaler = MaxAbsScaler()\n",
    "            scaler_name = \"MaxAbsScaler (sparse data detected)\"\n",
    "        elif len(outlier_cols) > len(numerical_cols) / 2:\n",
    "            self.scaler = RobustScaler()\n",
    "            scaler_name = \"RobustScaler (outliers detected)\"\n",
    "        elif len(skewed_cols) > len(numerical_cols) / 2:\n",
    "            self.scaler = MinMaxScaler()\n",
    "            scaler_name = \"MinMaxScaler (skewed data detected)\"\n",
    "        else:\n",
    "            self.scaler = StandardScaler()\n",
    "            scaler_name = \"StandardScaler (normal distribution assumed)\"\n",
    "        \n",
    "        print(f\"\\nüéØ Chosen scaler: {scaler_name}\")\n",
    "        \n",
    "        # Apply scaling\n",
    "        df_scaled = df.copy()\n",
    "        df_scaled[numerical_cols] = self.scaler.fit_transform(df[numerical_cols])\n",
    "        print(f\"   ‚úÖ Scaled {len(numerical_cols)} numerical columns\")\n",
    "        \n",
    "        return df_scaled\n",
    "\n",
    "    def auto_encode_categorical(self, df, target_col=None, max_categories=10):\n",
    "        \"\"\"Automatically encode categorical variables.\"\"\"\n",
    "        categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "        \n",
    "        if target_col and target_col in categorical_cols:\n",
    "            categorical_cols = categorical_cols.drop(target_col)\n",
    "        \n",
    "        if len(categorical_cols) == 0:\n",
    "            print(\"‚ÑπÔ∏è No categorical columns to encode.\")\n",
    "            return df\n",
    "        \n",
    "        print(f\"\\nüî§ Encoding {len(categorical_cols)} categorical columns...\")\n",
    "        df_encoded = df.copy()\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            unique_count = df[col].nunique()\n",
    "            \n",
    "            if unique_count <= max_categories:\n",
    "                # Use One-Hot Encoding for low cardinality\n",
    "                encoder = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "                encoded_data = encoder.fit_transform(df[[col]])\n",
    "                \n",
    "                # Create column names\n",
    "                feature_names = [f\"{col}_{cat}\" for cat in encoder.categories_[0][1:]]  # Skip first due to drop='first'\n",
    "                encoded_df = pd.DataFrame(encoded_data, columns=feature_names, index=df.index)\n",
    "                \n",
    "                # Add to main dataframe\n",
    "                df_encoded = pd.concat([df_encoded.drop(columns=[col]), encoded_df], axis=1)\n",
    "                self.encoders[col] = encoder\n",
    "                print(f\"   ‚úÖ One-hot encoded '{col}' ({unique_count} categories)\")\n",
    "                \n",
    "            else:\n",
    "                # Use Ordinal Encoding for high cardinality\n",
    "                encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "                df_encoded[col] = encoder.fit_transform(df[[col]]).flatten()\n",
    "                self.encoders[col] = encoder\n",
    "                print(f\"   ‚úÖ Ordinal encoded '{col}' ({unique_count} categories)\")\n",
    "        \n",
    "        # Handle target column separately if specified\n",
    "        if target_col and target_col in df.columns:\n",
    "            if df[target_col].dtype == 'object':\n",
    "                le = LabelEncoder()\n",
    "                df_encoded[target_col] = le.fit_transform(df[target_col])\n",
    "                self.encoders[target_col] = le\n",
    "                print(f\"   ‚úÖ Label encoded target column '{target_col}'\")\n",
    "        \n",
    "        return df_encoded\n",
    "\n",
    "    def remove_duplicates(self, df):\n",
    "        \"\"\"Remove duplicate rows.\"\"\"\n",
    "        initial_shape = df.shape[0]\n",
    "        df_cleaned = df.drop_duplicates()\n",
    "        removed_count = initial_shape - df_cleaned.shape[0]\n",
    "        \n",
    "        if removed_count > 0:\n",
    "            print(f\"üóëÔ∏è Removed {removed_count} duplicate rows\")\n",
    "        else:\n",
    "            print(\"‚úÖ No duplicates found\")\n",
    "            \n",
    "        return df_cleaned\n",
    "\n",
    "    def preprocess_pipeline(self, target_col=None):\n",
    "        \"\"\"Complete preprocessing pipeline.\"\"\"\n",
    "        print(\"üöÄ Starting Data Preprocessing Pipeline...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Step 1: Load data\n",
    "        df = self.get_dataset()\n",
    "        if df is None:\n",
    "            return None\n",
    "        \n",
    "        # Step 2: Sanity check\n",
    "        if not self.sanity_check(df):\n",
    "            return None\n",
    "        \n",
    "        # Step 3: Check categorical distributions\n",
    "        self.check_categorical_distributions(df)\n",
    "        \n",
    "        # Step 4: EDA summary\n",
    "        self.eda_summary(df)\n",
    "        \n",
    "        # Step 5: Remove duplicates\n",
    "        df = self.remove_duplicates(df)\n",
    "        \n",
    "        # Step 6: Handle missing values\n",
    "        df = self.handle_missing_values(df)\n",
    "        \n",
    "        # Step 7: Detect outliers\n",
    "        outlier_cols = self.detect_and_report_outliers(df)\n",
    "        \n",
    "        # Step 8: Scale numerical features\n",
    "        df = self.choose_and_apply_scaler(df)\n",
    "        \n",
    "        # Step 9: Encode categorical features\n",
    "        df = self.auto_encode_categorical(df, target_col=target_col)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"‚úÖ PREPROCESSING COMPLETED!\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Final dataset shape: {df.shape}\")\n",
    "        print(f\"Final columns: {list(df.columns)}\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize preprocessor\n",
    "    preprocessor = DataPreprocessor()\n",
    "    \n",
    "    # Run complete pipeline\n",
    "    # If you have a target column, specify it: target_col='your_target_column'\n",
    "    processed_df = preprocessor.preprocess_pipeline(target_col=None)\n",
    "    \n",
    "    if processed_df is not None:\n",
    "        print(\"\\nüéâ Data preprocessing successful!\")\n",
    "        print(\"Your data is now ready for machine learning!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Preprocessing failed. Please check your data and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f9dca8-811f-4982-802b-4d16692d4ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_methods(n_features, n_samples, has_target):\n",
    "    \"\"\"\n",
    "    Recommend feature selection/modeling approaches based on dataset characteristics.\n",
    "    \n",
    "    Args:\n",
    "        n_features (int): Number of features\n",
    "        n_samples (int): Number of samples\n",
    "        has_target (bool): Whether target variable exists (supervised)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Recommended methods grouped by type\n",
    "    \"\"\"\n",
    "    recommendations = {\n",
    "        \"feature_selection\": [],\n",
    "        \"modeling\": []\n",
    "    }\n",
    "    \n",
    "    if not has_target:\n",
    "        # Unsupervised methods / filter methods\n",
    "        recommendations[\"feature_selection\"].append(\"Variance Threshold\")\n",
    "        recommendations[\"feature_selection\"].append(\"Unsupervised clustering-based selection\")\n",
    "        # For modeling (unsupervised)\n",
    "        recommendations[\"modeling\"].append(\"KMeans\")\n",
    "        recommendations[\"modeling\"].append(\"PCA\")\n",
    "        recommendations[\"modeling\"].append(\"Autoencoders\")\n",
    "    \n",
    "    else:\n",
    "        # Supervised\n",
    "        \n",
    "        # Feature selection method choice\n",
    "        if n_features > 100 or n_samples > 5000:\n",
    "            # Large dataset ‚Äî prefer fast filter methods\n",
    "            recommendations[\"feature_selection\"].append(\"Variance Threshold\")\n",
    "            recommendations[\"feature_selection\"].append(\"Generic Univariate Feature Selection (e.g., SelectKBest with chi2, ANOVA)\")\n",
    "            recommendations[\"feature_selection\"].append(\"Mutual Information\")\n",
    "            # Modeling options for big data\n",
    "            recommendations[\"modeling\"].append(\"Linear / Logistic Regression\")\n",
    "            recommendations[\"modeling\"].append(\"Ridge Regression\")\n",
    "            recommendations[\"modeling\"].append(\"Random Forest (for embedded feature importance)\")\n",
    "            recommendations[\"modeling\"].append(\"XGBoost / LightGBM\")\n",
    "        \n",
    "        else:\n",
    "            # Smaller dataset ‚Äî can try wrapper and embedded methods\n",
    "            recommendations[\"feature_selection\"].append(\"Recursive Feature Elimination (RFE) with SVM or Random Forest\")\n",
    "            recommendations[\"feature_selection\"].append(\"Sequential Feature Selector (SFS)\")\n",
    "            recommendations[\"feature_selection\"].append(\"Generic Univariate Feature Selection\")\n",
    "            \n",
    "            # Modeling\n",
    "            recommendations[\"modeling\"].append(\"SVM (SVC / SVR)\")\n",
    "            recommendations[\"modeling\"].append(\"K-Nearest Neighbors (KNN)\")\n",
    "            recommendations[\"modeling\"].append(\"Linear / Logistic Regression\")\n",
    "            recommendations[\"modeling\"].append(\"Ridge Regression\")\n",
    "            recommendations[\"modeling\"].append(\"Random Forest\")\n",
    "            recommendations[\"modeling\"].append(\"Gradient Boosting Machines (XGBoost, LightGBM)\")\n",
    "    \n",
    "    return recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ebbb92e-6ceb-4bf6-a8c4-76a503f7079c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the name of the dataset:  iris.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully loaded dataset: iris.csv\n",
      "üöÄ Starting preprocessing pipeline...\n",
      "\n",
      "============================================================\n",
      "üìä DATA SANITY CHECK REPORT\n",
      "============================================================\n",
      "Shape of the dataset: (150, 5)\n",
      "Memory usage: 0.01 MB\n",
      "\n",
      "üìã Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sepal_length  150 non-null    float64\n",
      " 1   sepal_width   150 non-null    float64\n",
      " 2   petal_length  150 non-null    float64\n",
      " 3   petal_width   150 non-null    float64\n",
      " 4   species       150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n",
      "\n",
      "üîç Missing Values Analysis:\n",
      "‚úÖ No missing values found!\n",
      "\n",
      "üîÑ Duplicate rows: 3\n",
      "   (2.00% of total data)\n",
      "‚úÖ No missing values to handle!\n",
      "\n",
      "‚ö†Ô∏è Outlier Detection Results:\n",
      "     Column  Outlier Count  Outlier %\n",
      "sepal_width              4       2.67\n",
      "\n",
      "‚öôÔ∏è Scaling data using RobustScaler\n",
      "\n",
      "üî¢ Encoding Categorical Columns...\n",
      "   - One-hot encoded 'species' with 2 new columns\n",
      "‚úÖ Preprocessing complete!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter target column name (or press Enter if none):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üí° RECOMMENDED METHODS\n",
      "============================================================\n",
      "\n",
      "üîπ Feature Selection Methods:\n",
      " - Variance Threshold\n",
      " - Unsupervised clustering-based selection\n",
      "\n",
      "üîπ Modeling Methods:\n",
      " - KMeans\n",
      " - PCA\n",
      " - Autoencoders\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.imputer = None\n",
    "        self.scaler = None\n",
    "        self.encoders = {}\n",
    "        self.numerical_cols = []\n",
    "        self.categorical_cols = []\n",
    "        \n",
    "    def recommend_methods(self, n_features, n_samples, has_target):\n",
    "        \"\"\"\n",
    "        Recommend feature selection/modeling approaches based on dataset characteristics.\n",
    "        \n",
    "        Args:\n",
    "            n_features (int): Number of features\n",
    "            n_samples (int): Number of samples\n",
    "            has_target (bool): Whether target variable exists (supervised)\n",
    "        \n",
    "        Returns:\n",
    "            dict: Recommended methods grouped by type\n",
    "        \"\"\"\n",
    "        recommendations = {\n",
    "            \"feature_selection\": [],\n",
    "            \"modeling\": []\n",
    "        }\n",
    "        \n",
    "        if not has_target:\n",
    "            # Unsupervised methods / filter methods\n",
    "            recommendations[\"feature_selection\"].append(\"Variance Threshold\")\n",
    "            recommendations[\"feature_selection\"].append(\"Unsupervised clustering-based selection\")\n",
    "            # For modeling (unsupervised)\n",
    "            recommendations[\"modeling\"].append(\"KMeans\")\n",
    "            recommendations[\"modeling\"].append(\"PCA\")\n",
    "            recommendations[\"modeling\"].append(\"Autoencoders\")\n",
    "        \n",
    "        else:\n",
    "            # Supervised\n",
    "            \n",
    "            # Feature selection method choice\n",
    "            if n_features > 100 or n_samples > 5000:\n",
    "                # Large dataset ‚Äî prefer fast filter methods\n",
    "                recommendations[\"feature_selection\"].append(\"Variance Threshold\")\n",
    "                recommendations[\"feature_selection\"].append(\"Generic Univariate Feature Selection (e.g., SelectKBest with chi2, ANOVA)\")\n",
    "                recommendations[\"feature_selection\"].append(\"Mutual Information\")\n",
    "                # Modeling options for big data\n",
    "                recommendations[\"modeling\"].append(\"Linear / Logistic Regression\")\n",
    "                recommendations[\"modeling\"].append(\"Ridge Regression\")\n",
    "                recommendations[\"modeling\"].append(\"Random Forest (for embedded feature importance)\")\n",
    "                recommendations[\"modeling\"].append(\"XGBoost / LightGBM\")\n",
    "            \n",
    "            else:\n",
    "                # Smaller dataset ‚Äî can try wrapper and embedded methods\n",
    "                recommendations[\"feature_selection\"].append(\"Recursive Feature Elimination (RFE) with SVM or Random Forest\")\n",
    "                recommendations[\"feature_selection\"].append(\"Sequential Feature Selector (SFS)\")\n",
    "                recommendations[\"feature_selection\"].append(\"Generic Univariate Feature Selection\")\n",
    "                \n",
    "                # Modeling\n",
    "                recommendations[\"modeling\"].append(\"SVM (SVC / SVR)\")\n",
    "                recommendations[\"modeling\"].append(\"K-Nearest Neighbors (KNN)\")\n",
    "                recommendations[\"modeling\"].append(\"Linear / Logistic Regression\")\n",
    "                recommendations[\"modeling\"].append(\"Ridge Regression\")\n",
    "                recommendations[\"modeling\"].append(\"Random Forest\")\n",
    "                recommendations[\"modeling\"].append(\"Gradient Boosting Machines (XGBoost, LightGBM)\")\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "    def get_dataset(self):\n",
    "        \"\"\"Load dataset from CSV file.\"\"\"\n",
    "        try:\n",
    "            dataset = input(\"Enter the name of the dataset: \")\n",
    "            df = pd.read_csv(dataset)\n",
    "            print(f\"‚úÖ Successfully loaded dataset: {dataset}\")\n",
    "            return df\n",
    "        except FileNotFoundError:\n",
    "            print(\"‚ùå File not found. Please check the filename.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "    def sanity_check(self, df):\n",
    "        \"\"\"Perform comprehensive data quality checks.\"\"\"\n",
    "        if df is None:\n",
    "            print(\"‚ùå Data doesn't exist.\")\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"üìä DATA SANITY CHECK REPORT\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            # Basic info\n",
    "            print(f\"Shape of the dataset: {df.shape}\")\n",
    "            print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "            \n",
    "            print(\"\\nüìã Dataset Info:\")\n",
    "            df.info()\n",
    "            \n",
    "            # Missing values\n",
    "            print(\"\\nüîç Missing Values Analysis:\")\n",
    "            missing_percent = (df.isnull().sum() / df.shape[0] * 100).round(2)\n",
    "            missing_data = pd.DataFrame({\n",
    "                'Column': missing_percent.index,\n",
    "                'Missing Count': df.isnull().sum(),\n",
    "                'Missing %': missing_percent\n",
    "            })\n",
    "            missing_data = missing_data[missing_data['Missing Count'] > 0]\n",
    "            \n",
    "            if len(missing_data) > 0:\n",
    "                print(missing_data.to_string(index=False))\n",
    "            else:\n",
    "                print(\"‚úÖ No missing values found!\")\n",
    "            \n",
    "            # Duplicates\n",
    "            duplicate_count = df.duplicated().sum()\n",
    "            print(f\"\\nüîÑ Duplicate rows: {duplicate_count}\")\n",
    "            if duplicate_count > 0:\n",
    "                print(f\"   ({duplicate_count/len(df)*100:.2f}% of total data)\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during sanity check: {e}\")\n",
    "            return False\n",
    "\n",
    "    def check_categorical_distributions(self, df):\n",
    "        \"\"\"Analyze categorical column distributions.\"\"\"\n",
    "        if df is None:\n",
    "            return\n",
    "            \n",
    "        obj_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "        if len(obj_cols) == 0:\n",
    "            print(\"‚ÑπÔ∏è No categorical columns found.\")\n",
    "            return\n",
    "            \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìà CATEGORICAL DISTRIBUTIONS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for col in obj_cols:\n",
    "            unique_vals = df[col].nunique()\n",
    "            print(f\"\\n'{col}' ‚Äî {unique_vals} unique value(s)\")\n",
    "            \n",
    "            if unique_vals <= 15:  # Show distributions for low cardinality\n",
    "                print(df[col].value_counts(dropna=False).head(10))\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è High cardinality - showing top 5 values:\")\n",
    "                print(df[col].value_counts().head(5))\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "    def eda_summary(self, df):\n",
    "        \"\"\"Generate exploratory data analysis summary.\"\"\"\n",
    "        if df is None: \n",
    "            print(\"‚ùå Data doesn't exist.\")\n",
    "            return \n",
    "            \n",
    "        try:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"üìä EXPLORATORY DATA ANALYSIS\")\n",
    "            print(\"=\"*60)\n",
    "            \n",
    "            # Numerical summary\n",
    "            numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "            if len(numerical_cols) > 0:\n",
    "                print(\"\\nüî¢ Numerical Columns Summary:\")\n",
    "                print(df[numerical_cols].describe().round(2))\n",
    "            \n",
    "            # Categorical summary\n",
    "            categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "            if len(categorical_cols) > 0:\n",
    "                print(\"\\nüìù Categorical Columns Summary:\")\n",
    "                print(df[categorical_cols].describe())\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Exception occurred: {e}\")\n",
    "\n",
    "    def handle_missing_values(self, df):\n",
    "        \"\"\"Handle missing values using appropriate strategies.\"\"\"\n",
    "        if df.isnull().sum().sum() == 0:\n",
    "            print(\"‚úÖ No missing values to handle!\")\n",
    "            return df\n",
    "            \n",
    "        print(\"\\nüîß Handling Missing Values...\")\n",
    "        \n",
    "        # Separate numerical and categorical columns\n",
    "        self.numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        self.categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "        \n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # Handle numerical columns\n",
    "        if self.numerical_cols:\n",
    "            num_imputer = SimpleImputer(strategy='median')  # More robust than mean\n",
    "            df_processed[self.numerical_cols] = num_imputer.fit_transform(df[self.numerical_cols])\n",
    "            print(f\"   ‚úÖ Imputed {len(self.numerical_cols)} numerical columns with median\")\n",
    "        \n",
    "        # Handle categorical columns\n",
    "        if self.categorical_cols:\n",
    "            cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "            df_processed[self.categorical_cols] = cat_imputer.fit_transform(df[self.categorical_cols])\n",
    "            print(f\"   ‚úÖ Imputed {len(self.categorical_cols)} categorical columns with mode\")\n",
    "        \n",
    "        return df_processed\n",
    "\n",
    "    def detect_and_report_outliers(self, df):\n",
    "        \"\"\"Detect outliers using IQR method.\"\"\"\n",
    "        outlier_summary = []\n",
    "        \n",
    "        for col in df.select_dtypes(include=[np.number]).columns:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "            outlier_percentage = len(outliers) / len(df) * 100\n",
    "            \n",
    "            if outlier_percentage > 1:  # More than 1% outliers\n",
    "                outlier_summary.append({\n",
    "                    'Column': col,\n",
    "                    'Outlier Count': len(outliers),\n",
    "                    'Outlier %': round(outlier_percentage, 2)\n",
    "                })\n",
    "        \n",
    "        if outlier_summary:\n",
    "            print(\"\\n‚ö†Ô∏è Outlier Detection Results:\")\n",
    "            outlier_df = pd.DataFrame(outlier_summary)\n",
    "            print(outlier_df.to_string(index=False))\n",
    "            return [item['Column'] for item in outlier_summary]\n",
    "        else:\n",
    "            print(\"‚úÖ No significant outliers found.\")\n",
    "            return []\n",
    "\n",
    "    def choose_and_apply_scaler(self, df):\n",
    "        \"\"\"Choose appropriate scaler based on data characteristics.\"\"\"\n",
    "        numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        if len(numerical_cols) == 0:\n",
    "            print(\"‚ÑπÔ∏è No numerical columns to scale.\")\n",
    "            return df\n",
    "        \n",
    "        # Analyze data characteristics\n",
    "        outlier_cols = []\n",
    "        sparse_cols = []\n",
    "        skewed_cols = []\n",
    "        \n",
    "        for col in numerical_cols:\n",
    "            # Check for outliers\n",
    "            Q1, Q3 = df[col].quantile([0.25, 0.75])\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()\n",
    "            if outliers > len(df) * 0.01:\n",
    "                outlier_cols.append(col)\n",
    "            \n",
    "            # Check for sparsity (mostly zeros)\n",
    "            zero_ratio = (df[col] == 0).sum() / len(df)\n",
    "            if zero_ratio > 0.5:\n",
    "                sparse_cols.append(col)\n",
    "            \n",
    "            # Check skewness\n",
    "            if abs(df[col].skew()) > 1:\n",
    "                skewed_cols.append(col)\n",
    "        \n",
    "        # Decide scaler\n",
    "        scaler_choice = None\n",
    "        if len(outlier_cols) > 0:\n",
    "            scaler_choice = \"RobustScaler\"\n",
    "            self.scaler = RobustScaler()\n",
    "        elif len(sparse_cols) > 0:\n",
    "            scaler_choice = \"MaxAbsScaler\"\n",
    "            self.scaler = MaxAbsScaler()\n",
    "        else:\n",
    "            scaler_choice = \"StandardScaler\"\n",
    "            self.scaler = StandardScaler()\n",
    "        \n",
    "        print(f\"\\n‚öôÔ∏è Scaling data using {scaler_choice}\")\n",
    "        \n",
    "        df_scaled = df.copy()\n",
    "        df_scaled[numerical_cols] = self.scaler.fit_transform(df[numerical_cols])\n",
    "        \n",
    "        return df_scaled\n",
    "\n",
    "    def encode_categorical(self, df):\n",
    "        \"\"\"Encode categorical columns.\"\"\"\n",
    "        df_encoded = df.copy()\n",
    "        self.categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "        \n",
    "        if len(self.categorical_cols) == 0:\n",
    "            print(\"‚ÑπÔ∏è No categorical columns to encode.\")\n",
    "            return df_encoded\n",
    "        \n",
    "        print(\"\\nüî¢ Encoding Categorical Columns...\")\n",
    "        \n",
    "        for col in self.categorical_cols:\n",
    "            unique_vals = df[col].nunique()\n",
    "            if unique_vals <= 2:\n",
    "                # Binary encoding\n",
    "                le = LabelEncoder()\n",
    "                df_encoded[col] = le.fit_transform(df[col])\n",
    "                self.encoders[col] = le\n",
    "                print(f\"   - Label encoded '{col}' (binary)\")\n",
    "            else:\n",
    "                # OneHot encoding (can be changed to ordinal if needed)\n",
    "                ohe = OneHotEncoder(sparse_output=False, drop='first')\n",
    "                transformed = ohe.fit_transform(df[[col]])\n",
    "                new_cols = [f\"{col}_{cat}\" for cat in ohe.categories_[0][1:]]\n",
    "                df_ohe = pd.DataFrame(transformed, columns=new_cols, index=df.index)\n",
    "                df_encoded = pd.concat([df_encoded.drop(columns=[col]), df_ohe], axis=1)\n",
    "                self.encoders[col] = ohe\n",
    "                print(f\"   - One-hot encoded '{col}' with {len(new_cols)} new columns\")\n",
    "        \n",
    "        return df_encoded\n",
    "\n",
    "    def preprocess(self, df):\n",
    "        \"\"\"Run full preprocessing pipeline.\"\"\"\n",
    "        if df is None:\n",
    "            print(\"‚ùå No data to preprocess.\")\n",
    "            return None\n",
    "        \n",
    "        print(\"üöÄ Starting preprocessing pipeline...\")\n",
    "        \n",
    "        # Sanity check\n",
    "        if not self.sanity_check(df):\n",
    "            return None\n",
    "        \n",
    "        # Handle missing values\n",
    "        df = self.handle_missing_values(df)\n",
    "        \n",
    "        # Detect outliers (report only)\n",
    "        self.detect_and_report_outliers(df)\n",
    "        \n",
    "        # Scale numerical\n",
    "        df = self.choose_and_apply_scaler(df)\n",
    "        \n",
    "        # Encode categorical\n",
    "        df = self.encode_categorical(df)\n",
    "        \n",
    "        print(\"‚úÖ Preprocessing complete!\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def recommend_after_preprocessing(self, df, target_col=None):\n",
    "        \"\"\"Recommend methods based on preprocessed data shape and presence of target.\"\"\"\n",
    "        n_samples, n_features = df.shape\n",
    "        \n",
    "        has_target = False\n",
    "        if target_col and target_col in df.columns:\n",
    "            has_target = True\n",
    "            # Exclude target from feature count\n",
    "            n_features -= 1\n",
    "        \n",
    "        recommendations = self.recommend_methods(n_features, n_samples, has_target)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üí° RECOMMENDED METHODS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(\"\\nüîπ Feature Selection Methods:\")\n",
    "        for method in recommendations['feature_selection']:\n",
    "            print(f\" - {method}\")\n",
    "            \n",
    "        print(\"\\nüîπ Modeling Methods:\")\n",
    "        for method in recommendations['modeling']:\n",
    "            print(f\" - {method}\")\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    dp = DataPreprocessor()\n",
    "    \n",
    "    df = dp.get_dataset()\n",
    "    if df is not None:\n",
    "        df_processed = dp.preprocess(df)\n",
    "        \n",
    "        # Optionally specify target column for recommendations\n",
    "        target_column = input(\"Enter target column name (or press Enter if none): \").strip()\n",
    "        target_column = target_column if target_column else None\n",
    "        \n",
    "        dp.recommend_after_preprocessing(df_processed, target_column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65587fda-b5bc-4135-bec2-5748ef915521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the name of the dataset:  iris.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully loaded dataset: iris.csv\n",
      "‚ÑπÔ∏è Auto-detected target column: 'species'\n",
      "\n",
      "Target column detected: 'species'\n",
      "Number of features: 4, Number of samples: 150\n",
      "\n",
      "============================================================\n",
      "üìä DATA SANITY CHECK REPORT\n",
      "============================================================\n",
      "Shape of the dataset: (150, 5)\n",
      "Memory usage: 0.01 MB\n",
      "\n",
      "üìã Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sepal_length  150 non-null    float64\n",
      " 1   sepal_width   150 non-null    float64\n",
      " 2   petal_length  150 non-null    float64\n",
      " 3   petal_width   150 non-null    float64\n",
      " 4   species       150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n",
      "\n",
      "üîç Missing Values Analysis:\n",
      "‚úÖ No missing values found!\n",
      "\n",
      "üîÑ Duplicate rows: 3\n",
      "   (2.00% of total data)\n",
      "\n",
      "============================================================\n",
      "üìà CATEGORICAL DISTRIBUTIONS\n",
      "============================================================\n",
      "\n",
      "'species' ‚Äî 3 unique value(s)\n",
      "species\n",
      "setosa        50\n",
      "versicolor    50\n",
      "virginica     50\n",
      "Name: count, dtype: int64\n",
      "----------------------------------------\n",
      "\n",
      "============================================================\n",
      "üìä EXPLORATORY DATA ANALYSIS\n",
      "============================================================\n",
      "\n",
      "üî¢ Numerical Columns Summary:\n",
      "       sepal_length  sepal_width  petal_length  petal_width\n",
      "count        150.00       150.00        150.00       150.00\n",
      "mean           5.84         3.05          3.76         1.20\n",
      "std            0.83         0.43          1.76         0.76\n",
      "min            4.30         2.00          1.00         0.10\n",
      "25%            5.10         2.80          1.60         0.30\n",
      "50%            5.80         3.00          4.35         1.30\n",
      "75%            6.40         3.30          5.10         1.80\n",
      "max            7.90         4.40          6.90         2.50\n",
      "\n",
      "üìù Categorical Columns Summary:\n",
      "       species\n",
      "count      150\n",
      "unique       3\n",
      "top     setosa\n",
      "freq        50\n",
      "‚úÖ No missing values to handle!\n",
      "\n",
      "‚ö†Ô∏è Outlier Detection Results:\n",
      "     Column  Outlier Count  Outlier %\n",
      "sepal_width              4       2.67\n",
      "\n",
      "‚öôÔ∏è Scaling data using RobustScaler\n",
      "\n",
      "üî¢ Encoding Categorical Columns...\n",
      "   - One-hot encoded 'species' with 3 unique values\n",
      "\n",
      "============================================================\n",
      "üí° RECOMMENDATIONS\n",
      "============================================================\n",
      "Feature Selection: ['Recursive Feature Elimination (RFE) with SVM or Random Forest', 'Sequential Feature Selector (SFS)', 'Generic Univariate Feature Selection']\n",
      "Modeling Techniques: ['SVM (SVC / SVR)', 'K-Nearest Neighbors (KNN)', 'Linear / Logistic Regression', 'Ridge Regression', 'Random Forest', 'Gradient Boosting Machines (XGBoost, LightGBM)']\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.imputer = None\n",
    "        self.scaler = None\n",
    "        self.encoders = {}\n",
    "        self.numerical_cols = []\n",
    "        self.categorical_cols = []\n",
    "        \n",
    "    def recommend_methods(self, n_features, n_samples, has_target):\n",
    "        recommendations = {\n",
    "            \"feature_selection\": [],\n",
    "            \"modeling\": []\n",
    "        }\n",
    "        \n",
    "        if not has_target:\n",
    "            recommendations[\"feature_selection\"].append(\"Variance Threshold\")\n",
    "            recommendations[\"feature_selection\"].append(\"Unsupervised clustering-based selection\")\n",
    "            recommendations[\"modeling\"].append(\"KMeans\")\n",
    "            recommendations[\"modeling\"].append(\"PCA\")\n",
    "            recommendations[\"modeling\"].append(\"Autoencoders\")\n",
    "        else:\n",
    "            if n_features > 100 or n_samples > 5000:\n",
    "                recommendations[\"feature_selection\"].append(\"Variance Threshold\")\n",
    "                recommendations[\"feature_selection\"].append(\"Generic Univariate Feature Selection (e.g., SelectKBest with chi2, ANOVA)\")\n",
    "                recommendations[\"feature_selection\"].append(\"Mutual Information\")\n",
    "                recommendations[\"modeling\"].append(\"Linear / Logistic Regression\")\n",
    "                recommendations[\"modeling\"].append(\"Ridge Regression\")\n",
    "                recommendations[\"modeling\"].append(\"Random Forest (for embedded feature importance)\")\n",
    "                recommendations[\"modeling\"].append(\"XGBoost / LightGBM\")\n",
    "            else:\n",
    "                recommendations[\"feature_selection\"].append(\"Recursive Feature Elimination (RFE) with SVM or Random Forest\")\n",
    "                recommendations[\"feature_selection\"].append(\"Sequential Feature Selector (SFS)\")\n",
    "                recommendations[\"feature_selection\"].append(\"Generic Univariate Feature Selection\")\n",
    "                recommendations[\"modeling\"].append(\"SVM (SVC / SVR)\")\n",
    "                recommendations[\"modeling\"].append(\"K-Nearest Neighbors (KNN)\")\n",
    "                recommendations[\"modeling\"].append(\"Linear / Logistic Regression\")\n",
    "                recommendations[\"modeling\"].append(\"Ridge Regression\")\n",
    "                recommendations[\"modeling\"].append(\"Random Forest\")\n",
    "                recommendations[\"modeling\"].append(\"Gradient Boosting Machines (XGBoost, LightGBM)\")\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "    def get_dataset(self):\n",
    "        try:\n",
    "            dataset = input(\"Enter the name of the dataset: \")\n",
    "            df = pd.read_csv(dataset)\n",
    "            print(f\"‚úÖ Successfully loaded dataset: {dataset}\")\n",
    "            return df\n",
    "        except FileNotFoundError:\n",
    "            print(\"‚ùå File not found. Please check the filename.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "    def detect_target_column(self, df):\n",
    "        \"\"\"Auto-detect target column based on heuristics.\"\"\"\n",
    "        common_targets = ['target', 'label', 'class', 'species', 'outcome', 'y']\n",
    "        for col in common_targets:\n",
    "            if col in df.columns:\n",
    "                print(f\"‚ÑπÔ∏è Auto-detected target column: '{col}'\")\n",
    "                return col\n",
    "        \n",
    "        # Find categorical columns\n",
    "        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        if len(categorical_cols) == 1:\n",
    "            print(f\"‚ÑπÔ∏è Auto-detected target column: '{categorical_cols[0]}' (only categorical column)\")\n",
    "            return categorical_cols[0]\n",
    "        \n",
    "        if len(categorical_cols) > 1:\n",
    "            unique_counts = df[categorical_cols].nunique()\n",
    "            target_candidate = unique_counts.idxmin()\n",
    "            print(f\"‚ÑπÔ∏è Auto-detected target column: '{target_candidate}' (categorical with least unique values)\")\n",
    "            return target_candidate\n",
    "        \n",
    "        # Default to last column if no categorical found\n",
    "        print(f\"‚ÑπÔ∏è Defaulting to last column as target: '{df.columns[-1]}'\")\n",
    "        return df.columns[-1]\n",
    "\n",
    "    def sanity_check(self, df):\n",
    "        if df is None:\n",
    "            print(\"‚ùå Data doesn't exist.\")\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"üìä DATA SANITY CHECK REPORT\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"Shape of the dataset: {df.shape}\")\n",
    "            print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "            print(\"\\nüìã Dataset Info:\")\n",
    "            df.info()\n",
    "            print(\"\\nüîç Missing Values Analysis:\")\n",
    "            missing_percent = (df.isnull().sum() / df.shape[0] * 100).round(2)\n",
    "            missing_data = pd.DataFrame({\n",
    "                'Column': missing_percent.index,\n",
    "                'Missing Count': df.isnull().sum(),\n",
    "                'Missing %': missing_percent\n",
    "            })\n",
    "            missing_data = missing_data[missing_data['Missing Count'] > 0]\n",
    "            if len(missing_data) > 0:\n",
    "                print(missing_data.to_string(index=False))\n",
    "            else:\n",
    "                print(\"‚úÖ No missing values found!\")\n",
    "            duplicate_count = df.duplicated().sum()\n",
    "            print(f\"\\nüîÑ Duplicate rows: {duplicate_count}\")\n",
    "            if duplicate_count > 0:\n",
    "                print(f\"   ({duplicate_count/len(df)*100:.2f}% of total data)\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during sanity check: {e}\")\n",
    "            return False\n",
    "\n",
    "    def check_categorical_distributions(self, df):\n",
    "        if df is None:\n",
    "            return\n",
    "        obj_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "        if len(obj_cols) == 0:\n",
    "            print(\"‚ÑπÔ∏è No categorical columns found.\")\n",
    "            return\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìà CATEGORICAL DISTRIBUTIONS\")\n",
    "        print(\"=\"*60)\n",
    "        for col in obj_cols:\n",
    "            unique_vals = df[col].nunique()\n",
    "            print(f\"\\n'{col}' ‚Äî {unique_vals} unique value(s)\")\n",
    "            if unique_vals <= 15:\n",
    "                print(df[col].value_counts(dropna=False).head(10))\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è High cardinality - showing top 5 values:\")\n",
    "                print(df[col].value_counts().head(5))\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "    def eda_summary(self, df):\n",
    "        if df is None:\n",
    "            print(\"‚ùå Data doesn't exist.\")\n",
    "            return\n",
    "        try:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"üìä EXPLORATORY DATA ANALYSIS\")\n",
    "            print(\"=\"*60)\n",
    "            numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "            if len(numerical_cols) > 0:\n",
    "                print(\"\\nüî¢ Numerical Columns Summary:\")\n",
    "                print(df[numerical_cols].describe().round(2))\n",
    "            categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "            if len(categorical_cols) > 0:\n",
    "                print(\"\\nüìù Categorical Columns Summary:\")\n",
    "                print(df[categorical_cols].describe())\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Exception occurred: {e}\")\n",
    "\n",
    "    def handle_missing_values(self, df):\n",
    "        if df.isnull().sum().sum() == 0:\n",
    "            print(\"‚úÖ No missing values to handle!\")\n",
    "            return df\n",
    "        print(\"\\nüîß Handling Missing Values...\")\n",
    "        self.numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        self.categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "        df_processed = df.copy()\n",
    "        if self.numerical_cols:\n",
    "            num_imputer = SimpleImputer(strategy='median')\n",
    "            df_processed[self.numerical_cols] = num_imputer.fit_transform(df[self.numerical_cols])\n",
    "            print(f\"   ‚úÖ Imputed {len(self.numerical_cols)} numerical columns with median\")\n",
    "        if self.categorical_cols:\n",
    "            cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "            df_processed[self.categorical_cols] = cat_imputer.fit_transform(df[self.categorical_cols])\n",
    "            print(f\"   ‚úÖ Imputed {len(self.categorical_cols)} categorical columns with mode\")\n",
    "        return df_processed\n",
    "\n",
    "    def detect_and_report_outliers(self, df):\n",
    "        outlier_summary = []\n",
    "        for col in df.select_dtypes(include=[np.number]).columns:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "            outlier_percentage = len(outliers) / len(df) * 100\n",
    "            if outlier_percentage > 1:\n",
    "                outlier_summary.append({\n",
    "                    'Column': col,\n",
    "                    'Outlier Count': len(outliers),\n",
    "                    'Outlier %': round(outlier_percentage, 2)\n",
    "                })\n",
    "        if outlier_summary:\n",
    "            print(\"\\n‚ö†Ô∏è Outlier Detection Results:\")\n",
    "            outlier_df = pd.DataFrame(outlier_summary)\n",
    "            print(outlier_df.to_string(index=False))\n",
    "            return [item['Column'] for item in outlier_summary]\n",
    "        else:\n",
    "            print(\"‚úÖ No significant outliers found.\")\n",
    "            return []\n",
    "\n",
    "    def choose_and_apply_scaler(self, df):\n",
    "        numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        if len(numerical_cols) == 0:\n",
    "            print(\"‚ÑπÔ∏è No numerical columns to scale.\")\n",
    "            return df\n",
    "        outlier_cols = []\n",
    "        sparse_cols = []\n",
    "        skewed_cols = []\n",
    "        for col in numerical_cols:\n",
    "            Q1, Q3 = df[col].quantile([0.25, 0.75])\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()\n",
    "            if outliers > len(df) * 0.01:\n",
    "                outlier_cols.append(col)\n",
    "            zero_ratio = (df[col] == 0).sum() / len(df)\n",
    "            if zero_ratio > 0.5:\n",
    "                sparse_cols.append(col)\n",
    "            if abs(df[col].skew()) > 1:\n",
    "                skewed_cols.append(col)\n",
    "        scaler_choice = None\n",
    "        if len(outlier_cols) > 0:\n",
    "            scaler_choice = \"RobustScaler\"\n",
    "            self.scaler = RobustScaler()\n",
    "        elif len(sparse_cols) > 0:\n",
    "            scaler_choice = \"MaxAbsScaler\"\n",
    "            self.scaler = MaxAbsScaler()\n",
    "        else:\n",
    "            scaler_choice = \"StandardScaler\"\n",
    "            self.scaler = StandardScaler()\n",
    "        print(f\"\\n‚öôÔ∏è Scaling data using {scaler_choice}\")\n",
    "        df_scaled = df.copy()\n",
    "        df_scaled[numerical_cols] = self.scaler.fit_transform(df[numerical_cols])\n",
    "        return df_scaled\n",
    "\n",
    "    def encode_categorical(self, df):\n",
    "        df_encoded = df.copy()\n",
    "        self.categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "        if len(self.categorical_cols) == 0:\n",
    "            print(\"‚ÑπÔ∏è No categorical columns to encode.\")\n",
    "            return df_encoded\n",
    "        print(\"\\nüî¢ Encoding Categorical Columns...\")\n",
    "        for col in self.categorical_cols:\n",
    "            unique_vals = df[col].nunique()\n",
    "            if unique_vals <= 2:\n",
    "                le = LabelEncoder()\n",
    "                df_encoded[col] = le.fit_transform(df[col])\n",
    "                self.encoders[col] = le\n",
    "                print(f\"   - Label encoded '{col}' (binary)\")\n",
    "            else:\n",
    "                ohe = OneHotEncoder(sparse_output=False, drop='first')\n",
    "                transformed = ohe.fit_transform(df[[col]])\n",
    "                new_cols = [f\"{col}_{cat}\" for cat in ohe.categories_[0][1:]]\n",
    "                df_ohe = pd.DataFrame(transformed, columns=new_cols, index=df.index)\n",
    "                df_encoded = pd.concat([df_encoded.drop(columns=[col]), df_ohe], axis=1)\n",
    "                self.encoders[col] = ohe\n",
    "                print(f\"   - One-hot encoded '{col}' with {unique_vals} unique values\")\n",
    "        return df_encoded\n",
    "\n",
    "    def preprocess(self, df):\n",
    "        if df is None:\n",
    "            return None\n",
    "        # Auto-detect target\n",
    "        target_col = self.detect_target_column(df)\n",
    "        # Identify feature columns (excluding target)\n",
    "        features = df.drop(columns=[target_col])\n",
    "        target = df[target_col]\n",
    "\n",
    "        print(f\"\\nTarget column detected: '{target_col}'\")\n",
    "        print(f\"Number of features: {features.shape[1]}, Number of samples: {df.shape[0]}\")\n",
    "\n",
    "        # Sanity check and EDA\n",
    "        if not self.sanity_check(df):\n",
    "            return None\n",
    "        self.check_categorical_distributions(df)\n",
    "        self.eda_summary(df)\n",
    "\n",
    "        # Handle missing values\n",
    "        df_imputed = self.handle_missing_values(df)\n",
    "\n",
    "        # Detect outliers\n",
    "        outlier_cols = self.detect_and_report_outliers(df_imputed)\n",
    "\n",
    "        # Scale numerical features\n",
    "        df_scaled = self.choose_and_apply_scaler(df_imputed)\n",
    "\n",
    "        # Encode categorical features\n",
    "        df_encoded = self.encode_categorical(df_scaled)\n",
    "\n",
    "        # Summary recommendations\n",
    "        has_target = target_col is not None and target_col in df.columns\n",
    "        recs = self.recommend_methods(\n",
    "            n_features=features.shape[1],\n",
    "            n_samples=df.shape[0],\n",
    "            has_target=has_target\n",
    "        )\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üí° RECOMMENDATIONS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Feature Selection: {recs['feature_selection']}\")\n",
    "        print(f\"Modeling Techniques: {recs['modeling']}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        return df_encoded, target_col\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    dp = DataPreprocessor()\n",
    "    df = dp.get_dataset()\n",
    "    if df is not None:\n",
    "        processed_df, target_col = dp.preprocess(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42941855-a012-4276-8bd6-ab687e64af58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the name of the dataset file (CSV):  iris.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded dataset: iris.csv\n",
      "Auto-detected target column: 'species'\n",
      "\n",
      "Target column detected: 'species'\n",
      "Number of features: 4, Number of samples: 150\n",
      "\n",
      "DATA SANITY CHECK REPORT\n",
      "Shape of the dataset: (150, 5)\n",
      "Memory usage: 0.01 MB\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sepal_length  150 non-null    float64\n",
      " 1   sepal_width   150 non-null    float64\n",
      " 2   petal_length  150 non-null    float64\n",
      " 3   petal_width   150 non-null    float64\n",
      " 4   species       150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n",
      "\n",
      "Missing Values Analysis:\n",
      "No missing values found.\n",
      "\n",
      "Duplicate rows: 3\n",
      "(2.00% of total data)\n",
      "\n",
      "CATEGORICAL DISTRIBUTIONS\n",
      "\n",
      "'species' ‚Äî 3 unique value(s)\n",
      "species\n",
      "setosa        50\n",
      "versicolor    50\n",
      "virginica     50\n",
      "Name: count, dtype: int64\n",
      "\n",
      "EXPLORATORY DATA ANALYSIS\n",
      "\n",
      "Numerical Columns Summary:\n",
      "       sepal_length  sepal_width  petal_length  petal_width\n",
      "count        150.00       150.00        150.00       150.00\n",
      "mean           5.84         3.05          3.76         1.20\n",
      "std            0.83         0.43          1.76         0.76\n",
      "min            4.30         2.00          1.00         0.10\n",
      "25%            5.10         2.80          1.60         0.30\n",
      "50%            5.80         3.00          4.35         1.30\n",
      "75%            6.40         3.30          5.10         1.80\n",
      "max            7.90         4.40          6.90         2.50\n",
      "\n",
      "Categorical Columns Summary:\n",
      "       species\n",
      "count      150\n",
      "unique       3\n",
      "top     setosa\n",
      "freq        50\n",
      "No missing values to handle.\n",
      "\n",
      "Outlier Detection Results:\n",
      "     Column  Outlier Count  Outlier %\n",
      "sepal_width              4       2.67\n",
      "\n",
      "Scaling data using RobustScaler\n",
      "\n",
      "Encoding Categorical Columns...\n",
      "One-hot encoded 'species' with 3 unique values\n",
      "\n",
      "RECOMMENDATIONS\n",
      "Feature Selection Methods: ['Recursive Feature Elimination (RFE) with SVM or Random Forest', 'Sequential Feature Selector (SFS)', 'Generic Univariate Feature Selection']\n",
      "Modeling Techniques: ['SVM (SVC / SVR)', 'K-Nearest Neighbors (KNN)', 'Linear / Logistic Regression', 'Ridge Regression', 'Random Forest', 'Gradient Boosting Machines (XGBoost, LightGBM)']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler, RobustScaler\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.imputer = None\n",
    "        self.scaler = None\n",
    "        self.encoders = {}\n",
    "        self.numerical_cols = []\n",
    "        self.categorical_cols = []\n",
    "        \n",
    "    def recommend_methods(self, n_features, n_samples, has_target):\n",
    "        recommendations = {\n",
    "            \"feature_selection\": [],\n",
    "            \"modeling\": []\n",
    "        }\n",
    "        \n",
    "        if not has_target:\n",
    "            recommendations[\"feature_selection\"].append(\"Variance Threshold\")\n",
    "            recommendations[\"feature_selection\"].append(\"Unsupervised clustering-based selection\")\n",
    "            recommendations[\"modeling\"].append(\"KMeans\")\n",
    "            recommendations[\"modeling\"].append(\"PCA\")\n",
    "            recommendations[\"modeling\"].append(\"Autoencoders\")\n",
    "        else:\n",
    "            if n_features > 100 or n_samples > 5000:\n",
    "                recommendations[\"feature_selection\"].append(\"Variance Threshold\")\n",
    "                recommendations[\"feature_selection\"].append(\"Generic Univariate Feature Selection (e.g., SelectKBest with chi2, ANOVA)\")\n",
    "                recommendations[\"feature_selection\"].append(\"Mutual Information\")\n",
    "                recommendations[\"modeling\"].append(\"Linear / Logistic Regression\")\n",
    "                recommendations[\"modeling\"].append(\"Ridge Regression\")\n",
    "                recommendations[\"modeling\"].append(\"Random Forest (for embedded feature importance)\")\n",
    "                recommendations[\"modeling\"].append(\"XGBoost / LightGBM\")\n",
    "            else:\n",
    "                recommendations[\"feature_selection\"].append(\"Recursive Feature Elimination (RFE) with SVM or Random Forest\")\n",
    "                recommendations[\"feature_selection\"].append(\"Sequential Feature Selector (SFS)\")\n",
    "                recommendations[\"feature_selection\"].append(\"Generic Univariate Feature Selection\")\n",
    "                recommendations[\"modeling\"].append(\"SVM (SVC / SVR)\")\n",
    "                recommendations[\"modeling\"].append(\"K-Nearest Neighbors (KNN)\")\n",
    "                recommendations[\"modeling\"].append(\"Linear / Logistic Regression\")\n",
    "                recommendations[\"modeling\"].append(\"Ridge Regression\")\n",
    "                recommendations[\"modeling\"].append(\"Random Forest\")\n",
    "                recommendations[\"modeling\"].append(\"Gradient Boosting Machines (XGBoost, LightGBM)\")\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "    def get_dataset(self):\n",
    "        try:\n",
    "            dataset = input(\"Enter the name of the dataset file (CSV): \")\n",
    "            df = pd.read_csv(dataset)\n",
    "            print(f\"Successfully loaded dataset: {dataset}\")\n",
    "            return df\n",
    "        except FileNotFoundError:\n",
    "            print(\"File not found. Please check the filename.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "    def detect_target_column(self, df):\n",
    "        \"\"\"Auto-detect target column based on heuristics.\"\"\"\n",
    "        common_targets = ['target', 'label', 'class', 'species', 'outcome', 'y']\n",
    "        for col in common_targets:\n",
    "            if col in df.columns:\n",
    "                print(f\"Auto-detected target column: '{col}'\")\n",
    "                return col\n",
    "        \n",
    "        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        if len(categorical_cols) == 1:\n",
    "            print(f\"Auto-detected target column: '{categorical_cols[0]}' (only categorical column)\")\n",
    "            return categorical_cols[0]\n",
    "        \n",
    "        if len(categorical_cols) > 1:\n",
    "            unique_counts = df[categorical_cols].nunique()\n",
    "            target_candidate = unique_counts.idxmin()\n",
    "            print(f\"Auto-detected target column: '{target_candidate}' (categorical with least unique values)\")\n",
    "            return target_candidate\n",
    "        \n",
    "        print(f\"Defaulting to last column as target: '{df.columns[-1]}'\")\n",
    "        return df.columns[-1]\n",
    "\n",
    "    def sanity_check(self, df):\n",
    "        if df is None:\n",
    "            print(\"Data doesn't exist.\")\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            print(\"\\nDATA SANITY CHECK REPORT\")\n",
    "            print(f\"Shape of the dataset: {df.shape}\")\n",
    "            print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "            print(\"\\nDataset Info:\")\n",
    "            df.info()\n",
    "            print(\"\\nMissing Values Analysis:\")\n",
    "            missing_percent = (df.isnull().sum() / df.shape[0] * 100).round(2)\n",
    "            missing_data = pd.DataFrame({\n",
    "                'Column': missing_percent.index,\n",
    "                'Missing Count': df.isnull().sum(),\n",
    "                'Missing %': missing_percent\n",
    "            })\n",
    "            missing_data = missing_data[missing_data['Missing Count'] > 0]\n",
    "            if len(missing_data) > 0:\n",
    "                print(missing_data.to_string(index=False))\n",
    "            else:\n",
    "                print(\"No missing values found.\")\n",
    "            duplicate_count = df.duplicated().sum()\n",
    "            print(f\"\\nDuplicate rows: {duplicate_count}\")\n",
    "            if duplicate_count > 0:\n",
    "                print(f\"({duplicate_count/len(df)*100:.2f}% of total data)\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error during sanity check: {e}\")\n",
    "            return False\n",
    "\n",
    "    def check_categorical_distributions(self, df):\n",
    "        if df is None:\n",
    "            return\n",
    "        obj_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "        if len(obj_cols) == 0:\n",
    "            print(\"No categorical columns found.\")\n",
    "            return\n",
    "        print(\"\\nCATEGORICAL DISTRIBUTIONS\")\n",
    "        for col in obj_cols:\n",
    "            unique_vals = df[col].nunique()\n",
    "            print(f\"\\n'{col}' ‚Äî {unique_vals} unique value(s)\")\n",
    "            if unique_vals <= 15:\n",
    "                print(df[col].value_counts(dropna=False).head(10))\n",
    "            else:\n",
    "                print(\"High cardinality - showing top 5 values:\")\n",
    "                print(df[col].value_counts().head(5))\n",
    "\n",
    "    def eda_summary(self, df):\n",
    "        if df is None:\n",
    "            print(\"Data doesn't exist.\")\n",
    "            return\n",
    "        try:\n",
    "            print(\"\\nEXPLORATORY DATA ANALYSIS\")\n",
    "            numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "            if len(numerical_cols) > 0:\n",
    "                print(\"\\nNumerical Columns Summary:\")\n",
    "                print(df[numerical_cols].describe().round(2))\n",
    "            categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "            if len(categorical_cols) > 0:\n",
    "                print(\"\\nCategorical Columns Summary:\")\n",
    "                print(df[categorical_cols].describe())\n",
    "        except Exception as e:\n",
    "            print(f\"Exception occurred: {e}\")\n",
    "\n",
    "    def handle_missing_values(self, df):\n",
    "        if df.isnull().sum().sum() == 0:\n",
    "            print(\"No missing values to handle.\")\n",
    "            return df\n",
    "        print(\"\\nHandling Missing Values...\")\n",
    "        self.numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        self.categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "        df_processed = df.copy()\n",
    "        if self.numerical_cols:\n",
    "            num_imputer = SimpleImputer(strategy='median')\n",
    "            df_processed[self.numerical_cols] = num_imputer.fit_transform(df[self.numerical_cols])\n",
    "            print(f\"Imputed {len(self.numerical_cols)} numerical columns with median\")\n",
    "        if self.categorical_cols:\n",
    "            cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "            df_processed[self.categorical_cols] = cat_imputer.fit_transform(df[self.categorical_cols])\n",
    "            print(f\"Imputed {len(self.categorical_cols)} categorical columns with mode\")\n",
    "        return df_processed\n",
    "\n",
    "    def detect_and_report_outliers(self, df):\n",
    "        outlier_summary = []\n",
    "        for col in df.select_dtypes(include=[np.number]).columns:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "            outlier_percentage = len(outliers) / len(df) * 100\n",
    "            if outlier_percentage > 1:\n",
    "                outlier_summary.append({\n",
    "                    'Column': col,\n",
    "                    'Outlier Count': len(outliers),\n",
    "                    'Outlier %': round(outlier_percentage, 2)\n",
    "                })\n",
    "        if outlier_summary:\n",
    "            print(\"\\nOutlier Detection Results:\")\n",
    "            outlier_df = pd.DataFrame(outlier_summary)\n",
    "            print(outlier_df.to_string(index=False))\n",
    "            return [item['Column'] for item in outlier_summary]\n",
    "        else:\n",
    "            print(\"No significant outliers found.\")\n",
    "            return []\n",
    "\n",
    "    def choose_and_apply_scaler(self, df):\n",
    "        numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        if len(numerical_cols) == 0:\n",
    "            print(\"No numerical columns to scale.\")\n",
    "            return df\n",
    "        outlier_cols = []\n",
    "        sparse_cols = []\n",
    "        skewed_cols = []\n",
    "        for col in numerical_cols:\n",
    "            Q1, Q3 = df[col].quantile([0.25, 0.75])\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()\n",
    "            if outliers > len(df) * 0.01:\n",
    "                outlier_cols.append(col)\n",
    "            zero_ratio = (df[col] == 0).sum() / len(df)\n",
    "            if zero_ratio > 0.5:\n",
    "                sparse_cols.append(col)\n",
    "            if abs(df[col].skew()) > 1:\n",
    "                skewed_cols.append(col)\n",
    "        scaler_choice = None\n",
    "        if len(outlier_cols) > 0:\n",
    "            scaler_choice = \"RobustScaler\"\n",
    "            self.scaler = RobustScaler()\n",
    "        elif len(sparse_cols) > 0:\n",
    "            scaler_choice = \"MaxAbsScaler\"\n",
    "            self.scaler = MaxAbsScaler()\n",
    "        else:\n",
    "            scaler_choice = \"StandardScaler\"\n",
    "            self.scaler = StandardScaler()\n",
    "        print(f\"\\nScaling data using {scaler_choice}\")\n",
    "        df_scaled = df.copy()\n",
    "        df_scaled[numerical_cols] = self.scaler.fit_transform(df[numerical_cols])\n",
    "        return df_scaled\n",
    "\n",
    "    def encode_categorical(self, df):\n",
    "        df_encoded = df.copy()\n",
    "        self.categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "        if len(self.categorical_cols) == 0:\n",
    "            print(\"No categorical columns to encode.\")\n",
    "            return df_encoded\n",
    "        print(\"\\nEncoding Categorical Columns...\")\n",
    "        for col in self.categorical_cols:\n",
    "            unique_vals = df[col].nunique()\n",
    "            if unique_vals <= 2:\n",
    "                le = LabelEncoder()\n",
    "                df_encoded[col] = le.fit_transform(df[col])\n",
    "                self.encoders[col] = le\n",
    "                print(f\"Label encoded '{col}' (binary)\")\n",
    "            else:\n",
    "                ohe = OneHotEncoder(sparse_output=False, drop='first')\n",
    "                transformed = ohe.fit_transform(df[[col]])\n",
    "                new_cols = [f\"{col}_{cat}\" for cat in ohe.categories_[0][1:]]\n",
    "                df_ohe = pd.DataFrame(transformed, columns=new_cols, index=df.index)\n",
    "                df_encoded = pd.concat([df_encoded.drop(columns=[col]), df_ohe], axis=1)\n",
    "                self.encoders[col] = ohe\n",
    "                print(f\"One-hot encoded '{col}' with {unique_vals} unique values\")\n",
    "        return df_encoded\n",
    "\n",
    "    def preprocess(self, df):\n",
    "        if df is None:\n",
    "            return None\n",
    "        target_col = self.detect_target_column(df)\n",
    "        features = df.drop(columns=[target_col])\n",
    "        target = df[target_col]\n",
    "\n",
    "        print(f\"\\nTarget column detected: '{target_col}'\")\n",
    "        print(f\"Number of features: {features.shape[1]}, Number of samples: {df.shape[0]}\")\n",
    "\n",
    "        if not self.sanity_check(df):\n",
    "            return None\n",
    "        self.check_categorical_distributions(df)\n",
    "        self.eda_summary(df)\n",
    "\n",
    "        df_imputed = self.handle_missing_values(df)\n",
    "        outlier_cols = self.detect_and_report_outliers(df_imputed)\n",
    "        df_scaled = self.choose_and_apply_scaler(df_imputed)\n",
    "        df_encoded = self.encode_categorical(df_scaled)\n",
    "\n",
    "        has_target = target_col is not None and target_col in df.columns\n",
    "        recs = self.recommend_methods(\n",
    "            n_features=features.shape[1],\n",
    "            n_samples=df.shape[0],\n",
    "            has_target=has_target\n",
    "        )\n",
    "        print(\"\\nRECOMMENDATIONS\")\n",
    "        print(f\"Feature Selection Methods: {recs['feature_selection']}\")\n",
    "        print(f\"Modeling Techniques: {recs['modeling']}\\n\")\n",
    "\n",
    "        return df_encoded, target_col\n",
    "\n",
    "# Usage example:\n",
    "if __name__ == \"__main__\":\n",
    "    dp = DataPreprocessor()\n",
    "    df = dp.get_dataset()\n",
    "    if df is not None:\n",
    "        processed_df, target_col = dp.preprocess(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf4279d-7c4c-46a6-af3e-9a933ffc7f41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
